import gym
import gym_rock_paper_scissors
from gym_rock_paper_scissors.fixed_agents.mixed_strategy_agent import MixedStrategyAgent
from gym_rock_paper_scissors.fixed_agents.neural_agent import NeuralAgent
from gym_rock_paper_scissors.fixed_agents.counter_agent import CounterAgent
import numpy as np

env = gym.make('RockPaperScissors-v0', payoff_rock_vs_scissors = 3)

LOG_INTERVAL = 10
MAX_EPISODE = 1000
LEARNING_PERIOD = 15
def main():
    running_reward = 0
    villain = NeuralAgent(name = 'Freddy')
    # villain = MixedStrategyAgent([1/3 - 1/20,1/3 + 1/10,1/3 - 1/20], 'Alice')
    # villain = CounterAgent("karl")
    hero = NeuralAgent(name = 'Amelia')
    # villain_1 = MixedStrategyAgent([1/2,1/2,0], 'bob')
    # villain_2 = MixedStrategyAgent([1/2,0,1/2], 'alice')
    for episode in range(MAX_EPISODE):
        verbose = False
        ob = env.reset()
        ep_reward = 0
        for i in range(0, 10):
            villain_action = villain.take_action(ob[0])
            smart_action = hero.take_action(ob[1])
            action = [villain_action, smart_action]
            ob, reward, done, info = env.step(action)
            hero.handle_experience(reward[1])
            if isinstance(villain, NeuralAgent):
                villain.handle_experience(reward[0])
            if isinstance(villain, CounterAgent):
                villain.handle_experience(ob[0], env)
            ep_reward += reward[1]
            if done:
                break
        
            if episode % 999 == 0 : 
                print('villain action:')
                print(villain_action)
                print("villain reward" + str(reward[0]))
                print('hero action:')
                print(smart_action)
                print("hero reward" + str(reward[1]))
        
        
        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward

        # perform backprop
        if episode % 100 == 0:
            verbose = True

        if isinstance(villain, NeuralAgent):
            if np.random.uniform() < 0.5:
                hero.learn_strategy(verbose)
            else:
                villain.learn_strategy(verbose)
        else:
            hero.learn_strategy(verbose)

        # log results
        if episode % LOG_INTERVAL == 0:
            print('Episode {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
                episode, ep_reward, running_reward))
            # probs, state_value = hero.peek_model(ob[1])
            # print(f"The bot has learnt to play with parameters action preferences {probs.tolist()}, and state value approximation {state_value.item()}")

            # if isinstance(villain, NeuralAgent):
            #     probs_op, state_value_op = villain.peek_model(ob[0])
            #     print(f"The opponent is playing with {probs_op.tolist()}, and state value approximation {state_value_op.item()}")
        
        


        # check if we have "solved" the rock-paper-scissors problem
        if running_reward > 9 or episode == MAX_EPISODE - 1:
            print("Solved! Running reward is now {} and "
                "the last episode runs to {} time steps!".format(running_reward, i))
            probs, state_value = hero.peek_model(ob[1])
            if isinstance(villain, NeuralAgent):
                probs_op, state_value_op = villain.peek_model(ob[0])
                print(f"The opponent is playing with {probs_op.tolist()}, and state value approximation {state_value_op.item()}")
            
            print(f"The bot has learnt to play with parameters action preferences {probs.tolist()}, and state value approximation {state_value.item()}")
            
            break

    print("Done!")

main()